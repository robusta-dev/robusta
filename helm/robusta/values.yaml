# most of this file is documented at https://docs.robusta.dev/master/user-guide/configuration.html
# if you have any questions, feel free to ask via github issues or Slack (see link on robusta.dev)

# NOTE: Do not use this file to install Robusta. It has missing values that need to be filled in. See the installation instructions instead https://docs.robusta.dev/master/installation.html

# -- (string) Override the full name of the chart.
fullnameOverride: ""

# playbook repositories
playbookRepos: {}

# sinks configurations
sinksConfig: []

# global parameters
clusterName: ""
clusterZone: ""

global:
  clusterDomain: "cluster.local"

automountServiceAccountToken: true

enableHolmesGPT: false

# see https://docs.robusta.dev/master/user-guide/configuration.html#global-config and https://docs.robusta.dev/master/configuration/additional-settings.html#global-config
globalConfig:
  check_prometheus_flags: true
  grafana_url: ""
  grafana_api_key: ""
  grafana_dashboard_uid: ""
  prometheus_url: ""
  account_id: ""
  signing_key: ""
  custom_annotations: []
  custom_severity_map: {}

# see https://docs.robusta.dev/master/user-guide/configuration/additional-settings.html#relabel-prometheus-alerts
alertRelabel: []

# safe actions to enable authenticated users to run
lightActions:
- related_pods
- prometheus_enricher
- add_silence
- delete_pod
- delete_silence
- get_silences
- logs_enricher
- pod_events_enricher
- deployment_events_enricher
- job_events_enricher
- job_pod_enricher
- get_resource_yaml
- node_cpu_enricher
- node_disk_analyzer
- node_running_pods_enricher
- node_allocatable_resources_enricher
- node_status_enricher
- node_graph_enricher
- oomkilled_container_graph_enricher
- pod_oom_killer_enricher
- oom_killer_enricher
- volume_analysis
- python_profiler
- pod_ps
- python_memory
- debugger_stack_trace
- python_process_inspector
- prometheus_alert
- alertmanager_alert
- create_pvc_snapshot
- resource_events_enricher
- delete_job
- list_resource_names
- node_dmesg_enricher
- status_enricher
- krr_scan
- popeye_scan
- handle_alertmanager_event
- drain
- cordon
- uncordon
- rollout_restart
- ask_holmes
- prometheus_all_available_metrics
- prometheus_get_series
- prometheus_get_label_names
- holmes_workload_health
- holmes_conversation
- holmes_issue_chat
- holmes_chat
- holmes_workload_chat

# install prometheus, alert-manager, and grafana along with Robusta?
enablePrometheusStack: false
enabledManagedConfiguration: false
enableServiceMonitors: true
monitorHelmReleases: true
argoRollouts: false

# disable messages routed by Robusta cloud
disableCloudRouting: false

# scale alerts processing.
# Used to support clusters with high load of alerts. When used, the runner will consume more memory
scaleAlertsProcessing: False

# Enable loading playbooks to a persistent volume
playbooksPersistentVolume: false
playbooksPersistentVolumeSize: 4Gi

# priority builtin playbooks for running before all playbooks
priorityBuiltinPlaybooks:
# playbooks for prometheus silencing
- name: "CommonPrometheusAlertsSilencer"
  triggers:
  - on_prometheus_alert:
      status: "all"
  actions:
  - name_silencer:
      names: ["Watchdog", "KubeSchedulerDown", "KubeControllerManagerDown", "InfoInhibitor"]

# Silences for small/local clusters
- name: "MiniClustersPrometheusAlertsSilencer"
  triggers:
  - on_prometheus_alert:
      status: "all"
      k8s_providers: ["Minikube", "Kind", "RancherDesktop"]
  actions:
  - name_silencer:
      names: ["etcdInsufficientMembers", "etcdMembersDown", "NodeClockNotSynchronising", "PrometheusTSDBCompactionsFailing"]

# Silences for specific providers
- name: "GKEProviderAlertsAlertsSilencer"
  triggers:
  - on_prometheus_alert:
      status: "all"
      k8s_providers: [ "GKE" ]
  actions:
  - name_silencer:
      names: [ "KubeletDown" ]

- name: "DONodeAgentCPUThrottlingHighAlertSilencer"
  triggers:
  - on_prometheus_alert:
      alert_name: CPUThrottlingHigh
      k8s_providers: [ "DigitalOcean" ]
      pod_name_prefix: "do-node-agent"
  actions:
  - silence_alert:
      log_silence: true

# Smart Silences
- name: "TargetDownSilencer"
  triggers:
  - on_prometheus_alert:
      alert_name: TargetDown
  actions:
  - target_down_dns_silencer: {}

# custom user playbooks
customPlaybooks: []

# load custom playbooks by name, which allows overriding them when using multiple Helm values files - learn more https://docs.robusta.dev/master/playbook-reference/defining-playbooks/playbook-basics.html#organizing-playbooks
namedCustomPlaybooks: {}


# builtin playbooks
builtinPlaybooks:
# playbooks for non-prometheus based monitoring
- name: "CrashLoopBackOff"
  triggers:
  - on_pod_crash_loop:
      restart_reason: "CrashLoopBackOff"
  actions:
  - report_crash_loop: {}

- name: "ImagePullBackOff"
  triggers:
  - on_image_pull_backoff: {}
  actions:
  - image_pull_backoff_reporter: {}

# playbooks for non-prometheus based monitoring that use prometheus for enrichment
- name: "PodEvicted"
  triggers:
  - on_pod_evicted: {}
  actions:
  - pod_evicted_enricher: {}
  - pod_events_enricher: {}
  - enrich_pod_with_node_events: {}

- name: "PodOOMKill"
  triggers:
  - on_pod_oom_killed:
      rate_limit: 3600
  actions:
  - pod_oom_killer_enricher:
      attach_logs: true
      container_memory_graph: true
      node_memory_graph: true
      dmesg_log: false
  stop: true

# playbooks for prometheus alerts enrichment
- name: "KubePodCrashLooping"
  triggers:
  - on_prometheus_alert:
      alert_name: KubePodCrashLooping
  actions:
  - logs_enricher: {}
  - pod_events_enricher: {}

- name: "PrometheusRuleFailures"
  triggers:
  - on_prometheus_alert:
        alert_name: PrometheusRuleFailures
  actions:
  - prometheus_rules_enricher: {}
  - logs_enricher:
      filter_regex: ".*Evaluating rule failed.*"

- name: "KubeCPUOvercommit"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeCPUOvercommit
  actions:
  - cpu_overcommited_enricher: {}
  - external_video_enricher:
      url: https://bit.ly/overcommit-cpu
      name: CPU Overcommited

- name: "KubeMemoryOvercommit"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeMemoryOvercommit
  actions:
  - memory_overcommited_enricher: {}
  - external_video_enricher:
      url: https://bit.ly/memory-overcommit
      name: Memory Overcommited

- name: "KubePodNotReady"
  triggers:
  - on_prometheus_alert:
      alert_name: KubePodNotReady
  actions:
  - logs_enricher: {}
  - pod_events_enricher: {}
  - pod_issue_investigator: {}

- name: "KubeContainerWaiting"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeContainerWaiting
  actions:
  - pod_issue_investigator: {}
  - pod_events_enricher: {}

- name: "KubeHpaReplicasMismatch"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeHpaReplicasMismatch
  actions:
  - hpa_mismatch_enricher: {}

- name: "KubeJobAlerts"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeJobFailed
  - on_prometheus_alert:
      alert_name: KubeJobCompletion
  - on_prometheus_alert:
      alert_name: KubeJobNotCompleted
  actions:
  - job_info_enricher: {}
  - job_events_enricher: {}
  - job_pod_enricher: {}

- name: "KubeAggregatedAPIDown"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeAggregatedAPIDown
  actions:
  - api_service_status_enricher: {}

- name: "KubeletTooManyPods"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeletTooManyPods
  actions:
  - node_pods_capacity_enricher: {}
  - alert_explanation_enricher:
      alert_explanation: "The node is approaching the maximum number of scheduled pods."
      recommended_resolution: "Verify that you defined proper resource requests for your workloads. If pods cannot be scheduled, add more nodes to your cluster."

- name: "KubeNodeNotReady"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeNodeNotReady
  actions:
  - node_allocatable_resources_enricher: {}
  - node_running_pods_enricher: {}
  - status_enricher:
      show_details: true

- name: "KubeNodeUnreachable"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeNodeUnreachable
  actions:
  - resource_events_enricher: {}
  - node_status_enricher: {}

# Prometheus Statefulset playbooks
- name: "KubeStatefulSetReplicasMismatch"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeStatefulSetReplicasMismatch
  actions:
  - resource_events_enricher:
      dependent_pod_mode: true
  - statefulset_replicas_enricher: {}
  - pod_issue_investigator: {}

- name: "KubeStatefulSetUpdateNotRolledOut"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeStatefulSetUpdateNotRolledOut
  actions:
  - related_pods: {}
  - statefulset_replicas_enricher: {}


# Prometheus Daemonset playbooks
- name: "KubeDaemonSetRolloutStuck"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeDaemonSetRolloutStuck
  actions:
  - resource_events_enricher: {}
  - related_pods: {}
  - daemonset_status_enricher: {}

- name: "K8sMisscheduled"
  triggers:
  - on_prometheus_alert:
      alert_name: KubernetesDaemonsetMisscheduled
  - on_prometheus_alert:
      alert_name: KubeDaemonSetMisScheduled
  actions:
  - daemonset_status_enricher: {}
  - daemonset_misscheduled_analysis_enricher: {}

- name: "HostHighCpuLoad"
  triggers:
  - on_prometheus_alert:
      alert_name: HostHighCpuLoad
  actions:
  - node_cpu_enricher: {}
  - alert_graph_enricher:
      resource_type: CPU
      item_type: Node

- name: "HostOomKillDetected"
  triggers:
  - on_prometheus_alert:
      alert_name: HostOomKillDetected
  actions:
  - oom_killer_enricher: {}
  - alert_graph_enricher:
      resource_type: Memory
      item_type: Node

- name: "NodeFSSpaceAlerts"
  triggers:
  - on_prometheus_alert:
      alert_name: NodeFilesystemSpaceFillingUp
  - on_prometheus_alert:
      alert_name: NodeFilesystemAlmostOutOfSpace
  actions:
  - node_disk_analyzer: {}
  - alert_graph_enricher:
      resource_type: Disk
      item_type: Node

- name: "CPUThrottlingHigh"
  triggers:
  - on_prometheus_alert:
      alert_name: CPUThrottlingHigh
      status: "all" # sometimes this enricher silences the alert, so we need to silence it regardless of status
  actions:
  - cpu_throttling_analysis_enricher: {}
  - alert_graph_enricher:
      resource_type: CPU
      item_type: Pod

- name: "DeploymentReplicasMismatch"
  triggers:
  - on_prometheus_alert:
      alert_name: KubernetesDeploymentReplicasMismatch
  - on_prometheus_alert:
      alert_name: KubeDeploymentReplicasMismatch
  actions:
  - pod_issue_investigator: {}
  - deployment_events_enricher:
      included_types: ["Warning"]
  - deployment_events_enricher:
      included_types: ["Warning", "Normal"]
      dependent_pod_mode: true

- name: "KubeVersionMismatch"
  triggers:
  - on_prometheus_alert:
      alert_name: KubeVersionMismatch
  actions:
  - version_mismatch_enricher: {}

- name: "DefaultPrometheusAlertEnricher"
  triggers:
  - on_prometheus_alert:
      status: "all"
  actions:
  - default_enricher: {}

- name: "MiniClusterNodeFSSpaceAlerts"
  triggers:
  - on_prometheus_alert:
      alert_name: NodeFilesystemSpaceFillingUp
      k8s_providers: ["Minikube", "Kind", "RancherDesktop"]
  - on_prometheus_alert:
      alert_name: NodeFilesystemAlmostOutOfSpace
      k8s_providers: ["Minikube", "Kind", "RancherDesktop"]
  actions:
  - alert_explanation_enricher:
      alert_explanation: "This alert is fired when the file system is running out of space."
      recommended_resolution: "This is a common issue on local clusters and we recommend increasing the node disk size for your cluster to run optimally."

# additional builtin playbooks to enable when using Robusta UI
# these are disabled by default without the UI because they are spammy when sent to slack

enablePlatformPlaybooks: false

platformPlaybooks:
- name: "K8sWarningEventsReport"
  triggers:
  - on_kubernetes_warning_event_create:
      exclude: ["NodeSysctlChange"]
  actions:
  - warning_events_report:
      warning_event_groups:
        - aggregation_key: PodLifecycleWarning
          matchers:
            - FailedCreatePodSandBox
            - FailedToRetrieveImagePullSecret
            - BackOff
            - FailedDaemonPod
            - FailedKillPod
            - FailedPreStopHook
            - ExceededGracePeriod
            - Evicted
        - aggregation_key: NodeHealthWarning
          matchers:
            - InvalidDiskCapacity
            - NodeNotReady
            - NodeRegistrationCheckerStart
            - WorkflowNodeFailed
            - OOMKilling
            - FailedDraining
            - PreemptScheduled
            - NodeRegistrationCheckerDidNotRunChecks
            - NodeShutdown
            - TerminateScheduled
            - FreeDiskSpaceFailed
            - NodeSysctlChange
            - SystemOOM
            - NodeStartupFailed
            - Drain
            - ScaleUpFailed
            - ScaleDownFailed
        - aggregation_key: ProbeFailure
          matchers:
            - Unhealthy
            - ProbeError
            - ProbeWarning
        - aggregation_key: PolicyViolation
          matchers:
            - PolicyViolation
        - aggregation_key: ScaleWarning
          matchers:
            - FailedGetResourceMetric
            - FailedComputeMetricsReplicas
            - KEDAScalerFailed
            - FailedGetContainerResourceMetric
            - FailedGetExternalMetric
            - ScaledObjectCheckFailed
            - FailedGetObjectMetric
        - aggregation_key: SchedulingWarning
          matchers:
            - FailedScheduling
        - aggregation_key: VolumeWarning
          matchers:
            - FailedMount
            - FailedAttachVolume
            - VolumeFailedDelete
  - event_resource_events: {}
  sinks:
    - "robusta_ui_sink"

- name: "K8sChangeTracking"
  triggers:
    - on_kubernetes_resource_operation:
        resources: ["deployment", "daemonset", "statefulset"]
  actions:
    - resource_babysitter: {}
  sinks:
    - "robusta_ui_sink"

- name: "IngressChangeTracking"
  triggers:
    - on_kubernetes_resource_operation:
        resources: ["ingress"]
  actions:
    - resource_babysitter: {}
    - customise_finding:
        aggregation_key: IngressChange
  sinks:
    - "robusta_ui_sink"

- name: "EventBasedChangeTracking"
  triggers:
    - on_kubernetes_resource_operation:
        resources: ["deployment", "replicaset", "daemonset", "statefulset", "pod", "node", "job" ]
  actions:
    - resource_events_diff: {}

- name: "K8sJobFailure"
  triggers:
  - on_job_failure: {}
  actions:
  - create_finding:
      aggregation_key: "JobFailure"
      title: "Job $name on namespace $namespace failed"
  - job_info_enricher: {}
  - job_events_enricher: {}
  - job_pod_enricher: {}
  sinks:
    - "robusta_ui_sink"

- name: "WeeklyKRRScan"
  triggers:
  - on_schedule:
      cron_schedule_repeat:
        cron_expression: "0 12 * * 1" # every Monday at 12:00
  actions:
  - krr_scan: {}
  sinks:
    - "robusta_ui_sink"

- name: "RobustaMaintenance"
  triggers:
  - on_schedule:
      cron_schedule_repeat:
        cron_expression: "0 */3 * * *" # every 3 hours
  actions:
  - cleanup_robusta_pods:
      hours_back: 6
  sinks:
    - "robusta_ui_sink"

# Any playbook name listed here will be disabled
disabledPlaybooks: []

image:
  registry: robustadev

# parameters for the robusta forwarder deployment
kubewatch:
  image: ~ # image can be used to override image.registry/imageName
  imageName: kubewatch:v2.9.0
  imagePullPolicy: IfNotPresent
  pprof: True
  resources:
    requests:
      cpu: 10m
      memory: 512Mi
    limits:
      cpu: ~
  additional_env_vars: []
  priorityClassName: ""
  tolerations: []
  annotations: {}
  nodeSelector: ~
  imagePullSecrets: []
  config:
    namespace: ""
    resource:
      deployment: true
      replicationcontroller: false  # 0.10.12 disabled because not supported on the runner
      replicaset: true
      daemonset: true
      statefulset: true
      services: true
      pod: true
      job: true
      node: true
      hpa: true
      clusterrole: true
      clusterrolebinding: true
      serviceaccount: true
      persistentvolume: true
      namespace: true
      configmap: true # 0.9.17
      secret: false       # disabled for security reasons
      event: true  # updated on kubewatch 2.5
      coreevent: false # added on kubewatch 2.5
      ingress: true # full support on kubewatch 2.4 (earlier versions have ingress bugs)
  securityContext:
    container:
      allowPrivilegeEscalation: false
      capabilities: {}
      privileged: false
      readOnlyRootFilesystem: false
      runAsUser: 1000
    pod: {}
  customServiceAccount: "" # to override the kubewatch service account
  serviceMonitor:
    path: /metrics
  serviceAccount:
    # Additional annotations for the ServiceAccount.
    annotations: {}

# parameters for the renderer service used in robusta runner to render grafana graphs
grafanaRenderer:
  enableContainer: false
  image: ~  # image can be used to override image.registry/imageName
  imageName: grafana-renderer:7
  imagePullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: ~
  securityContext:
    container:
      privileged: false

# parameters for the robusta runner service account
runnerServiceAccount:
  # image pull secrets added to the runner service account. Any pod using the service account will get those
  imagePullSecrets: []
  # Additional annotations for the ServiceAccount.
  annotations: {}

# parameters for the robusta runner
runner:
  image: ~  # image can be used to override image.registry/imageName
  imageName: robusta-runner:0.0.0
  imagePullPolicy: IfNotPresent
  log_level: INFO
  sentry_dsn: https://4f1a66f025c60830fec303a094dcdf94@o1120648.ingest.sentry.io/6156573
  sendAdditionalTelemetry: false
  certificate: "" # base64 encoded
  customServiceAccount: "" # to override the runner service account
  resources:
    requests:
      cpu: 250m
      memory: 1024Mi
    limits:
      cpu: ~
  additional_env_vars: []
  additional_env_froms: []
  priorityClassName: ""
  tolerations: []
  annotations: {}
  nodeSelector: ~
  customClusterRoleRules: []
  imagePullSecrets: []
  extraVolumes: []
  extraVolumeMounts: []
  # k8s service config
  service:
    # custom service annotations
    annotations: {}
  serviceMonitor:
    path: /metrics
  securityContext:
    container:
      allowPrivilegeEscalation: false
      capabilities: {}
      privileged: false
      readOnlyRootFilesystem: false
    pod: {}

kube-prometheus-stack:
  alertmanager:
    tplConfig: true
    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: [ '...' ]
        group_wait: 1s
        group_interval: 1s
        repeat_interval: 4h
        receiver: 'robusta'
        routes:
          - match:
              alertname: Watchdog
            receiver: 'null'
      receivers:
        - name: 'null'
        - name:  'robusta'
          webhook_configs:
            - url: 'http://robusta-runner.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/api/alerts'
              send_resolved: true
    alertmanagerSpec:
      resources:
        requests:
          cpu: 50m
          memory: 128Mi
        limits:
          memory: 128Mi
      storage:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
  kubeProxy:
    enabled: false
  prometheus:
    prometheusSpec:
      resources:
        requests:
          cpu: 50m
          memory: 2Gi
        limits:
          memory: 2Gi
      retention: 15d
      # we set a value slightly lower than the 100Gi below
      # the retentionSize uses the suffix GB but it is really Gi units
      # that is, the retentionSize is measured in base2 units just like Gi, Mi, etc
      retentionSize: "99GB"

      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 100Gi
  prometheus-node-exporter:
    service:
      port: 9104
      targetPort: 9104
    resources:
      requests:
        cpu: 50m
        memory: 50Mi
      limits:
        memory: 50Mi
    # disable node-exporter on fargate because fargate doesn't allow daemonsets
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                    - fargate
  prometheusOperator:
    resources:
      requests:
        cpu: 100m
    prometheusConfigReloader:
      resources:
        limits:
          cpu: 0
  kube-state-metrics:
    resources:
      requests:
        cpu: 10m
        memory: 256Mi
      limits:
        memory: 256Mi

# custom parameters for OpenShift clusters
openshift:
  enabled: false
  createScc: false
  createPrivilegedScc: false

  privilegedSccName: null
  sccName: null

  sccPriority: null
  privilegedSccPriority: null
