# most of this file is documented at https://docs.robusta.dev/master/user-guide/configuration.html
# if you have any questions, feel free to ask via github issues or Slack (see link on robusta.dev)

# playbook repositories
playbookRepos: {}

# sinks configurations
sinksConfig: []

# global parameters
clusterName: ""
clusterZone: ""

# see https://docs.robusta.dev/master/user-guide/configuration.html#global-config
globalConfig:
  grafana_url: ""
  grafana_api_key: ""
  grafana_dashboard_uid: ""
  prometheus_url: ""
  account_id: ""
  signing_key: ""

# install prometheus, alert-manager, and grafana along with Robusta?
enablePrometheusStack: false
enableServiceMonitors: false

# disable messages routed by Robusta cloud
disableCloudRouting: false

# Enable loading playbooks to a persistent volume
playbooksPersistentVolume: false
playbooksPersistentVolumeSize: 4Gi

# priority builtin playbooks for running before all playbooks
priorityBuiltinPlaybooks:
# playbooks for prometheus silencing
- triggers:
  - on_prometheus_alert:
      status: "all"
  actions:
  - name_silencer:
      names: ["Watchdog", "KubeSchedulerDown", "KubeControllerManagerDown", "InfoInhibitor"]

# custom user playbooks
customPlaybooks: []

# builtin playbooks
builtinPlaybooks:
# playbooks for non-prometheus based monitoring
- triggers:
  - on_pod_crash_loop:
      restart_reason: "CrashLoopBackOff"
  actions:
  - report_crash_loop: {}
- triggers:
  - on_pod_update: {}
  actions:
  - image_pull_backoff_reporter:
      rate_limit: 3600

# playbooks for prometheus alerts enrichment
- triggers:
  - on_prometheus_alert:
      alert_name: KubePodCrashLooping
  actions:
  - logs_enricher: {}

- triggers:
  - on_prometheus_alert:
      alert_name: KubeNodeNotReady
  actions:
  - node_allocatable_resources_enricher: {}
  - node_running_pods_enricher: {}
- triggers:
  - on_prometheus_alert:
      alert_name: KubernetesDaemonsetMisscheduled
  - on_prometheus_alert:
      alert_name: KubeDaemonSetMisScheduled
  actions:
  - daemonset_status_enricher: {}
  - daemonset_misscheduled_analysis_enricher: {}
- triggers:
  - on_prometheus_alert:
      alert_name: HostHighCpuLoad
  actions:
  - node_cpu_enricher: {}
  - alert_graph_enricher:
      resource_type: CPU
      item_type: Node
- triggers:
  - on_prometheus_alert:
      alert_name: HostOomKillDetected
  actions:
  - oom_killer_enricher: {}
  - alert_graph_enricher:
      resource_type: Memory
      item_type: Node
- triggers:
  - on_prometheus_alert:
      alert_name: NodeFilesystemSpaceFillingUp
  actions:
  - node_disk_analyzer: {}
  - alert_graph_enricher:
      resource_type: Disk
      item_type: Node
- triggers:
  - on_prometheus_alert:
      alert_name: CPUThrottlingHigh
  actions:
  - cpu_throttling_analysis_enricher: {}
  - alert_graph_enricher:
      resource_type: CPU
      item_type: Pod
- triggers:
  - on_prometheus_alert:
      alert_name: KubernetesDeploymentReplicasMismatch
  - on_prometheus_alert:
      alert_name: KubeDeploymentReplicasMismatch
  actions:
  - deployment_status_enricher: {}
- triggers:
  - on_prometheus_alert:
      status: "all"
  actions:
  - default_enricher: {}

# additional builtin playbooks to enable when using Robusta UI
# these are disabled by default without the UI because they are spammy when sent to slack

enablePlatformPlaybooks: false

platformPlaybooks:
- triggers:
  - on_kubernetes_warning_event_create: {}
  actions:
  - event_report: {}
  - event_resource_events: {}
  sinks:
    - "robusta_ui_sink"
- triggers:
    - on_deployment_all_changes: {}
    - on_daemonset_all_changes: {}
    - on_statefulset_all_changes: {}
  actions:
    - resource_babysitter: {}
  sinks:
    - "robusta_ui_sink"
- triggers:
    - on_configmap_all_changes: {}
  actions:
    - resource_babysitter:
        fields_to_monitor: ["ConfigMap.data"]
        ignored_namespaces: ["kube-system"] # kube-system configmaps updated a lot (spammy)
  sinks:
    - "robusta_ui_sink"

# parameters for the robusta forwarder deployment
kubewatch:
  image: us-central1-docker.pkg.dev/genuine-flight-317411/devel/kubewatch:v1.11
  imagePullPolicy: IfNotPresent
  pprof: True
  resources:
    requests:
      cpu: 10m
      memory: 512Mi
    limits:
      cpu: ~
  tolerations: []
  annotations: {}

# parameters for the renderer service used in robusta runner to render grafana graphs
grafanaRenderer:
  image: us-central1-docker.pkg.dev/genuine-flight-317411/devel/grafana-renderer:6
  imagePullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: ~

# parameters for the robusta runner
runner:
  image: us-central1-docker.pkg.dev/genuine-flight-317411/devel/robusta-runner:0.0.0
  imagePullPolicy: IfNotPresent
  log_level: INFO
  sentry_dsn: https://53b627690db14de7b02095407596fa16@o1120648.ingest.sentry.io/6156573
  sendAdditionalTelemetry: false
  resources:
    requests:
      cpu: 250m
      memory: 1024Mi
    limits:
      cpu: ~
  additional_env_vars: []
  tolerations: []
  annotations: {}

kube-prometheus-stack:
  alertmanager:
    tplConfig: true
    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: [ 'job' ]
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 4h
        receiver: 'robusta'
        routes:
          - match:
              alertname: Watchdog
            receiver: 'null'
      receivers:
        - name: 'null'
        - name:  'robusta'
          webhook_configs:
            - url: 'http://robusta-runner.{{ .Release.Namespace }}.svc.cluster.local/api/alerts'
              send_resolved: true
    alertmanagerSpec:
      resources:
        requests:
          cpu: 50m
  kubeProxy:
    enabled: false
  prometheus:
    prometheusSpec:
      resources:
        requests:
          cpu: 50m
      retention: 14d
      # we set a value slightly lower than the 100Gi below
      # the retentionSize uses the suffix GB but it is really Gi units
      # that is, the retentionSize is measured in base2 units just like Gi, Mi, etc
      retentionSize: "99GB"

      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 100Gi
  prometheus-node-exporter:
    service:
      port: 9104
      targetPort: 9104
    resources:
      requests:
        cpu: 50m
  prometheusOperator:
    resources:
      requests:
        cpu: 100m
    prometheusConfigReloader:
      resources:
        limits:
          cpu: 0
  kube-state-metrics:
    resources:
      requests:
        cpu: 10m

rsa: ~

